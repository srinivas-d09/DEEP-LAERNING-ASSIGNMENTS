# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WjY8ytFa2rbcCxF5Wcm5AmVP99s5yPaF
"""

import torch
import torchvision
from torchvision.transforms import functional as F
import cv2
import numpy as np
from google.colab.patches import cv2_imshow
from google.colab import files
import time

# Load the pre-trained Faster R-CNN model
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model.eval()

# Define COCO object categories
COCO_INSTANCE_CATEGORY_NAMES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle',
    'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',
    'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',
    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',
    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',
    'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant',
    'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven',
    'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]

# Function to upload images
def upload_images():
    uploaded = files.upload()
    file_paths = []
    for fn in uploaded.keys():
        file_paths.append(fn)
    return file_paths

# Function to perform object detection
def detect_objects(image_path, confidence_threshold=0.5):
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_tensor = F.to_tensor(img_rgb).unsqueeze(0)

    # Perform inference without calculating gradients
    with torch.no_grad():
        predictions = model(img_tensor)

    filtered_predictions = []
    for i in range(len(predictions[0]['boxes'])):
        label_index = int(predictions[0]['labels'][i])
        # Ensure the label index is within the valid range
        label_index = min(label_index, len(COCO_INSTANCE_CATEGORY_NAMES) - 1)

        # Only consider detections above the confidence threshold
        if predictions[0]['scores'][i] >= confidence_threshold:
            filtered_predictions.append({
                'box': predictions[0]['boxes'][i].tolist(),
                'label': COCO_INSTANCE_CATEGORY_NAMES[label_index],
                'score': predictions[0]['scores'][i].item()
            })

    return filtered_predictions

# Function to annotate and display image
def annotate_and_display_image(image_path, detections):
    img = cv2.imread(image_path)

    # Annotate the image with bounding boxes and labels
    for detection in detections:
        print(f"Detected {detection['label']} with confidence {detection['score']:.2f} at {detection['box']}")
        x1, y1, x2, y2 = map(int, detection['box'])

        # Draw bounding box
        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)

        # Add label with confidence score
        label_with_confidence = f"{detection['label']} ({detection['score']:.2f})"
        cv2.putText(img, label_with_confidence, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    # Convert image back to BGR for displaying with OpenCV
    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)

    # Display the image with annotations (bounding boxes and labels)
    cv2_imshow(img_bgr)

    # Save the image with annotations for download
    output_image_path = 'annotated_' + image_path
    cv2.imwrite(output_image_path, img)
    return output_image_path

# Main function to process multiple images
def process_images(confidence_threshold=0.5):
    # Upload images
    image_paths = upload_images()

    # Process each image
    for image_path in image_paths:
        # Measure time for processing
        start_time = time.time()

        # Perform object detection
        detections = detect_objects(image_path, confidence_threshold)

        # Annotate and display the image with bounding boxes
        output_image_path = annotate_and_display_image(image_path, detections)

        # Measure and print the time taken for processing the image
        elapsed_time = time.time() - start_time
        print(f"Processed {image_path} in {elapsed_time:.2f} seconds.")

        # Provide download link for the annotated image
        files.download(output_image_path)

# Run the processing function with a confidence threshold of 0.5
process_images(confidence_threshold=0.6)

import torch
import torchvision
from torchvision.transforms import functional as F
import cv2
import numpy as np
from google.colab.patches import cv2_imshow
from google.colab import files
import time
import pandas as pd

# Load the pre-trained Faster R-CNN model
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model.eval()

# Define COCO object categories
COCO_INSTANCE_CATEGORY_NAMES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle',
    'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',
    'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',
    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',
    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',
    'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant',
    'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven',
    'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]

# Function to upload images
def upload_images():
    uploaded = files.upload()
    file_paths = []
    for fn in uploaded.keys():
        file_paths.append(fn)
    return file_paths

# Function to perform object detection
def detect_objects(image_path, confidence_threshold=0.5):
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_tensor = F.to_tensor(img_rgb).unsqueeze(0)

    # Perform inference without calculating gradients
    with torch.no_grad():
        predictions = model(img_tensor)

    filtered_predictions = []
    for i in range(len(predictions[0]['boxes'])):
        label_index = int(predictions[0]['labels'][i])
        # Ensure the label index is within the valid range
        label_index = min(label_index, len(COCO_INSTANCE_CATEGORY_NAMES) - 1)

        # Only consider detections above the confidence threshold
        if predictions[0]['scores'][i] >= confidence_threshold:
            filtered_predictions.append({
                'box': predictions[0]['boxes'][i].tolist(),
                'label': COCO_INSTANCE_CATEGORY_NAMES[label_index],
                'score': predictions[0]['scores'][i].item()
            })

    return filtered_predictions

# Function to annotate and display image
def annotate_and_display_image(image_path, detections):
    img = cv2.imread(image_path)

    # Annotate the image with bounding boxes and labels
    for detection in detections:
        print(f"Detected {detection['label']} with confidence {detection['score']:.2f} at {detection['box']}")
        x1, y1, x2, y2 = map(int, detection['box'])

        # Draw bounding box
        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)

        # Add label with confidence score
        label_with_confidence = f"{detection['label']} ({detection['score']:.2f})"
        cv2.putText(img, label_with_confidence, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    # Convert image back to BGR for displaying with OpenCV
    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)

    # Display the image with annotations (bounding boxes and labels)
    cv2_imshow(img_bgr)

    # Save the image with annotations for download
    output_image_path = 'annotated_' + image_path
    cv2.imwrite(output_image_path, img)
    return output_image_path

# Function to summarize detections
def summarize_detections(detections):
    object_count = {}
    for detection in detections:
        label = detection['label']
        if label not in object_count:
            object_count[label] = 1
        else:
            object_count[label] += 1
    return object_count

# Function to display summary table
def display_summary_table(object_count):
    # Convert the dictionary to a pandas DataFrame
    df = pd.DataFrame(object_count.items(), columns=['Object', 'Count'])
    # Display the table
    print("\nDetection Summary:")
    print(df)

# Main function to process multiple images
def process_images(confidence_threshold=0.5):
    # Upload images
    image_paths = upload_images()

    # Initialize a global dictionary to hold overall object counts
    global_object_count = {}

    # Process each image
    for image_path in image_paths:
        # Measure time for processing
        start_time = time.time()

        # Perform object detection
        detections = detect_objects(image_path, confidence_threshold)

        # Summarize the detections for the current image
        object_count = summarize_detections(detections)

        # Add the counts to the global dictionary
        for obj, count in object_count.items():
            if obj not in global_object_count:
                global_object_count[obj] = count
            else:
                global_object_count[obj] += count

        # Annotate and display the image with bounding boxes
        output_image_path = annotate_and_display_image(image_path, detections)

        # Measure and print the time taken for processing the image
        elapsed_time = time.time() - start_time
        print(f"Processed {image_path} in {elapsed_time:.2f} seconds.")

        # Provide download link for the annotated image
        files.download(output_image_path)

    # After processing all images, display the overall summary table
    display_summary_table(global_object_count)

# Run the processing function with a confidence threshold of 0.6
process_images(confidence_threshold=0.6)

import torch
import torchvision
from torchvision import models, transforms
import cv2
import numpy as np
from google.colab.patches import cv2_imshow
from google.colab import files
import time
import pandas as pd

# Load pre-trained Faster R-CNN model
model_faster_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model_faster_rcnn.eval()

# Load a pre-trained scene recognition model (e.g., ResNet-50 trained on Places365 dataset)
model_scene = models.resnet50(pretrained=True)
model_scene.eval()

# Define image transformations for the scene classification model
scene_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize(256),
    transforms.CenterCrop(224),
])

# Define COCO object categories for Faster R-CNN
COCO_INSTANCE_CATEGORY_NAMES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',
    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',
    'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',
    'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
    'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]

# Function to perform object detection
def detect_objects(image_path, confidence_threshold=0.5):
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_tensor = F.to_tensor(img_rgb).unsqueeze(0)

    with torch.no_grad():
        predictions = model_faster_rcnn(img_tensor)

    filtered_predictions = []
    for i in range(len(predictions[0]['boxes'])):
        label_index = int(predictions[0]['labels'][i])
        label_index = min(label_index, len(COCO_INSTANCE_CATEGORY_NAMES) - 1)

        if predictions[0]['scores'][i] >= confidence_threshold:
            filtered_predictions.append({
                'box': predictions[0]['boxes'][i].tolist(),
                'label': COCO_INSTANCE_CATEGORY_NAMES[label_index],
                'score': predictions[0]['scores'][i].item()
            })

    return filtered_predictions

# Function for scene classification
def classify_scene(image_path):
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_tensor = scene_transform(img_rgb).unsqueeze(0)

    with torch.no_grad():
        outputs = model_scene(img_tensor)

    _, predicted_class = torch.max(outputs, 1)

    # Assuming you have a class-to-label mapping for Places365
    scene_class_map = {  # Just an example
        0: "Indoors", 1: "Urban", 2: "Mountain", 3: "Beach", 4: "Forest", 5: "Desert"
    }

    scene_label = scene_class_map.get(predicted_class.item(), "Unknown")
    return scene_label

# Function to summarize detections
def summarize_detections(detections):
    object_count = {}
    for detection in detections:
        label = detection['label']
        if label not in object_count:
            object_count[label] = 1
        else:
            object_count[label] += 1
    return object_count

# Main function to process multiple images
def process_images(confidence_threshold=0.5):
    image_paths = upload_images()
    global_object_count = {}

    for image_path in image_paths:
        start_time = time.time()

        # Perform object detection
        detections = detect_objects(image_path, confidence_threshold)

        # Classify the scene (e.g., urban, beach)
        scene_type = classify_scene(image_path)
        print(f"Scene classified as: {scene_type}")

        # Summarize object detections
        object_count = summarize_detections(detections)

        for obj, count in object_count.items():
            if obj not in global_object_count:
                global_object_count[obj] = count
            else:
                global_object_count[obj] += count

        # Annotate and display the image with bounding boxes
        output_image_path = annotate_and_display_image(image_path, detections)

        elapsed_time = time.time() - start_time
        print(f"Processed {image_path} in {elapsed_time:.2f} seconds.")

        files.download(output_image_path)

    # After processing all images, display the overall summary table
    display_summary_table(global_object_count)

# Run the function
process_images(confidence_threshold=0.6)

import torch
import torchvision
from torchvision import models, transforms
import cv2
import numpy as np
from google.colab.patches import cv2_imshow
from google.colab import files
import time
import pandas as pd

# Load pre-trained Faster R-CNN model
model_faster_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model_faster_rcnn.eval()

# Load a pre-trained scene recognition model (e.g., ResNet-50 trained on Places365 dataset)
model_scene = models.resnet50(pretrained=True)
model_scene.eval()

# Define image transformations for the scene classification model
scene_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize(256),
    transforms.CenterCrop(224),
])

# Define COCO object categories for Faster R-CNN
COCO_INSTANCE_CATEGORY_NAMES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',
    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',
    'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',
    'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
    'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]

# Function to perform object detection
def detect_objects(image_path, confidence_threshold=0.5):
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_tensor = F.to_tensor(img_rgb).unsqueeze(0)

    with torch.no_grad():
        predictions = model_faster_rcnn(img_tensor)

    filtered_predictions = []
    for i in range(len(predictions[0]['boxes'])):
        label_index = int(predictions[0]['labels'][i])
        label_index = min(label_index, len(COCO_INSTANCE_CATEGORY_NAMES) - 1)

        if predictions[0]['scores'][i] >= confidence_threshold:
            filtered_predictions.append({
                'box': predictions[0]['boxes'][i].tolist(),
                'label': COCO_INSTANCE_CATEGORY_NAMES[label_index],
                'score': predictions[0]['scores'][i].item()
            })

    return filtered_predictions

# Function for scene classification
def classify_scene(image_path):
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_tensor = scene_transform(img_rgb).unsqueeze(0)

    with torch.no_grad():
        outputs = model_scene(img_tensor)

    _, predicted_class = torch.max(outputs, 1)

    # Assuming you have a class-to-label mapping for Places365
    scene_class_map = {  # Just an example
        0: "Indoors", 1: "Urban", 2: "Mountain", 3: "Beach", 4: "Forest", 5: "Desert"
    }

    scene_label = scene_class_map.get(predicted_class.item(), "Unknown")
    return scene_label

# Function to summarize detections
def summarize_detections(detections):
    object_count = {}
    for detection in detections:
        label = detection['label']
        if label not in object_count:
            object_count[label] = 1
        else:
            object_count[label] += 1
    return object_count

# Main function to process multiple images
def process_images(confidence_threshold=0.5):
    image_paths = upload_images()
    global_object_count = {}

    for image_path in image_paths:
        start_time = time.time()

        # Perform object detection
        detections = detect_objects(image_path, confidence_threshold)

        # Classify the scene (e.g., urban, beach)
        scene_type = classify_scene(image_path)
        print(f"Scene classified as: {scene_type}")

        # Summarize object detections
        object_count = summarize_detections(detections)

        for obj, count in object_count.items():
            if obj not in global_object_count:
                global_object_count[obj] = count
            else:
                global_object_count[obj] += count

        # Annotate and display the image with bounding boxes
        output_image_path = annotate_and_display_image(image_path, detections)

        elapsed_time = time.time() - start_time
        print(f"Processed {image_path} in {elapsed_time:.2f} seconds.")

        files.download(output_image_path)

    # After processing all images, display the overall summary table
    display_summary_table(global_object_count)

# Run the function
process_images(confidence_threshold=0.6)

import torch
import torchvision
from torchvision import models, transforms
import cv2
import numpy as np
from google.colab.patches import cv2_imshow
from google.colab import files
import time
import pandas as pd
import os # Import the os module


# ... (rest of your code) ...

def detect_objects(image_path, confidence_threshold=0.5):
    # Check if the image file exists
    if not os.path.exists(image_path):
        print(f"Error: Image file not found at {image_path}")
        return []  # Return an empty list to indicate failure

    img = cv2.imread(image_path)

    # Check if image loading was successful
    if img is None:
        print(f"Error: Could not load image from {image_path}")
        return []

    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_tensor = F.to_tensor(img_rgb).unsqueeze(0)
# Load pre-trained Faster R-CNN model
model_faster_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model_faster_rcnn.eval()

# Load a pre-trained scene recognition model (e.g., ResNet-50 trained on Places365 dataset)
model_scene = models.resnet50(pretrained=True)
model_scene.eval()

# Define image transformations for the scene classification model
scene_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize(256),
    transforms.CenterCrop(224),
])

# Define COCO object categories for Faster R-CNN
COCO_INSTANCE_CATEGORY_NAMES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',
    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',
    'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',
    'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
    'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]

# Function to perform object detection
def detect_objects(image_path, confidence_threshold=0.5):
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_tensor = F.to_tensor(img_rgb).unsqueeze(0)

    with torch.no_grad():
        predictions = model_faster_rcnn(img_tensor)

    filtered_predictions = []
    for i in range(len(predictions[0]['boxes'])):
        label_index = int(predictions[0]['labels'][i])
        label_index = min(label_index, len(COCO_INSTANCE_CATEGORY_NAMES) - 1)

        if predictions[0]['scores'][i] >= confidence_threshold:
            filtered_predictions.append({
                'box': predictions[0]['boxes'][i].tolist(),
                'label': COCO_INSTANCE_CATEGORY_NAMES[label_index],
                'score': predictions[0]['scores'][i].item()
            })

    return filtered_predictions

# Function for scene classification
def classify_scene(image_path):
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_tensor = scene_transform(img_rgb).unsqueeze(0)

    with torch.no_grad():
        outputs = model_scene(img_tensor)

    _, predicted_class = torch.max(outputs, 1)

    # Assuming you have a class-to-label mapping for Places365
    scene_class_map = {  # Just an example
        0: "Indoors", 1: "Urban", 2: "Mountain", 3: "Beach", 4: "Forest", 5: "Desert"
    }

    scene_label = scene_class_map.get(predicted_class.item(), "Unknown")
    return scene_label

# Function to summarize detections
def summarize_detections(detections):
    object_count = {}
    for detection in detections:
        label = detection['label']
        if label not in object_count:
            object_count[label] = 1
        else:
            object_count[label] += 1
    return object_count

# Main function to process multiple images
def process_images(confidence_threshold=0.5):
    image_paths = upload_images()
    global_object_count = {}

    for image_path in image_paths:
        start_time = time.time()

        # Perform object detection
        detections = detect_objects(image_path, confidence_threshold)

        # Classify the scene (e.g., urban, beach)
        scene_type = classify_scene(image_path)
        print(f"Scene classified as: {scene_type}")

        # Summarize object detections
        object_count = summarize_detections(detections)

        for obj, count in object_count.items():
            if obj not in global_object_count:
                global_object_count[obj] = count
            else:
                global_object_count[obj] += count

        # Annotate and display the image with bounding boxes
        output_image_path = annotate_and_display_image(image_path, detections)

        elapsed_time = time.time() - start_time
        print(f"Processed {image_path} in {elapsed_time:.2f} seconds.")

        files.download(output_image_path)

    # After processing all images, display the overall summary table
    display_summary_table(global_object_count)

# Run the function
process_images(confidence_threshold=0.6)